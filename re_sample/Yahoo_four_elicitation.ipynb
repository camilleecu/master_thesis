{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c86d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\"))  \n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064955c7-6535-4302-a999-57c927b4874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pysparnn.cluster_index as ci\n",
    "# import scipy.sparse\n",
    "# import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "from utils_elicitation import train_test_split, df_to_matrix ,matrix_to_df_2, threshold_interactions_df, matrix_to_df,set_intersection,get_0_and_p_index,set_diff, matrix_to_full_df, threshold_interactions_df_plus, train_test_split_csr\n",
    "\n",
    "!pip install surprise\n",
    "from surprise import Reader, accuracy\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pct.tree.heuristic.Heuristic import Heuristic5\n",
    "from pct.tree.heuristic.NumericHeuristic import NumericHeuristic5\n",
    "from pct.tree.splitter.splitter import Splitter\n",
    "from pct.tree.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15794805-3524-4078-8ac0-6e260b1ee251",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_csv(\"../Yahoodata/filtered_df.csv\")\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90a0d4-120b-4ddd-b563-b442775936a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# item type map\n",
    "item_type_map = filtered_df.drop_duplicates(subset='item_id')[['item_id', 'item_type']]\n",
    "item_type_map = dict(zip(item_type_map['item_id'], item_type_map['item_type']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf9fac-677d-44ae-82f5-9144bdcf2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_type(item_id):\n",
    "    return item_type_map.get(item_id, 'unknown')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1e96b-cf4a-4df9-8ea1-8c09a6740405",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_item_type(238709))  \n",
    "print(get_item_type(141799))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_user_ids = sorted(filtered_df['user_id'].unique().tolist())\n",
    "\n",
    "def split_users_by_ratio(all_user_ids, ratio):\n",
    "    n = len(all_user_ids)\n",
    "    split_point = int(n * ratio)\n",
    "    warm_users = all_user_ids[:split_point]\n",
    "    cold_users = all_user_ids[split_point:]\n",
    "    return warm_users, cold_users\n",
    "\n",
    "# Example ratios from 10% to 50%\n",
    "# ratios = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "# splits = {r: split_users_by_ratio(all_user_ids, r) for r in ratios}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4056d4",
   "metadata": {},
   "source": [
    "## 10% warm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_users_idx, cold_users_idx = split_users_by_ratio(all_user_ids, 0.1)\n",
    "\n",
    "df_warm = filtered_df[filtered_df['user_id'].isin(warm_users_idx)].copy()\n",
    "df_cold = filtered_df[filtered_df['user_id'].isin(cold_users_idx)].copy()\n",
    "\n",
    "matrix_warm, rid_to_idx_warm, idx_to_rid_warm, cid_to_idx, idx_to_cid = df_to_matrix(\n",
    "    df_warm, \"user_id\", \"item_id\", \"rating\")\n",
    "\n",
    "\n",
    "matrix_cold, rid_to_idx_cold, idx_to_rid_cold, _, _ = df_to_matrix( \n",
    "    df_cold, \"user_id\", \"item_id\", \"rating\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_combine(strategy=\"artist-only\"):\n",
    "    \"\"\"Handles both approaches with proper matrix alignment\"\"\"\n",
    "    # Get full cold matrix and mappings\n",
    "    matrix_cold, rid_to_idx_cold, _, cid_to_idx, _ = df_to_matrix(\n",
    "        df_cold, \"user_id\", \"item_id\", \"rating\"\n",
    "    )\n",
    "    matrix_cold = matrix_cold.tocsr()\n",
    "\n",
    "    # Create boolean masks\n",
    "    artist_mask = np.isin(\n",
    "        np.arange(matrix_cold.shape[1]), \n",
    "        [cid_to_idx[iid] for iid in df_cold[df_cold['item_type'] == 'artist']['item_id']]\n",
    "    )\n",
    "    genre_mask = ~artist_mask\n",
    "\n",
    "    # Create aligned matrices\n",
    "    matrix_cold_artist = matrix_cold.multiply(artist_mask)\n",
    "    matrix_cold_genre = matrix_cold.multiply(genre_mask)\n",
    "    matrix_cold_artist = matrix_cold.multiply(artist_mask).tocsr()\n",
    "    matrix_cold_genre = matrix_cold.multiply(genre_mask).tocsr()\n",
    "\n",
    "    al_artist, test_cold, _ = train_test_split(\n",
    "        matrix_cold_artist, \n",
    "        split_count=30,\n",
    "        fraction=None\n",
    "    )\n",
    "    \n",
    "    if strategy == \"artist-only\":\n",
    "        X_cold, K_cold, _ = train_test_split_csr(al_artist, 1)  \n",
    "        return K_cold, X_cold, test_cold\n",
    "    \n",
    "    elif strategy == \"hybrid\":\n",
    "        X_cold, K_cold, _ = train_test_split_csr(al_artist, 1)\n",
    "        X_cold = X_cold + matrix_cold_genre\n",
    "        return K_cold, X_cold, test_cold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa0837",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 7\n",
    "random.seed(my_seed)\n",
    "np.random.seed(my_seed)\n",
    "\n",
    "train_cold_K_artist, X_cold_artist, test_cold_artist  = split_and_combine(\"artist-only\")\n",
    "train_cold_K_hybrid, X_cold_hybrid, test_cold_hybrid = split_and_combine(\"hybrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb1f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of test_cold\n",
    "\n",
    "print(f\"Cold users in X: {len(np.unique(X_cold_artist.nonzero()[0]))}\")\n",
    "print(f\"Cold items in X: {len(np.unique(X_cold_artist.nonzero()[1]))}\")\n",
    "print(f\"test users in test: {len(np.unique(test_cold_artist.nonzero()[0]))}\")\n",
    "print(f\"test items in test: {len(np.unique(test_cold_artist.nonzero()[1]))}\")\n",
    "print(f\"train users in train: {len(np.unique(train_cold_K_artist.nonzero()[0]))}\")\n",
    "print(f\"train items in train: {len(np.unique(train_cold_K_artist.nonzero()[1]))}\")\n",
    "\n",
    "print(\"-------------------\")\n",
    "\n",
    "# shape of test_cold\n",
    "print(f\"Shape of test_cold: {test_cold_artist.shape}\")\n",
    "print(f\"Shape of train_cold: {train_cold_K_artist.shape}\")\n",
    "print(f\"Shape of X_cold: {X_cold_artist.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc19bf4-a23c-4b33-b45c-751ac90f83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cold users in X: {len(np.unique(X_cold_hybrid.nonzero()[0]))}\")\n",
    "print(f\"Cold items in X: {len(np.unique(X_cold_hybrid.nonzero()[1]))}\")\n",
    "print(f\"test users in test: {len(np.unique(test_cold_hybrid.nonzero()[0]))}\")\n",
    "print(f\"test items in test: {len(np.unique(test_cold_hybrid.nonzero()[1]))}\")\n",
    "print(f\"train users in train: {len(np.unique(train_cold_K_hybrid.nonzero()[0]))}\")\n",
    "print(f\"train items in train: {len(np.unique(train_cold_K_hybrid.nonzero()[1]))}\")\n",
    "\n",
    "print(\"-------------------\")\n",
    "\n",
    "# shape of test_cold\n",
    "print(f\"Shape of test_cold: {test_cold_hybrid.shape}\")\n",
    "print(f\"Shape of train_cold: {train_cold_K_hybrid.shape}\")\n",
    "print(f\"Shape of X_cold: {X_cold_hybrid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff78f92",
   "metadata": {},
   "source": [
    "Cold users in X: 1531\n",
    "Cold items in X: 4793\n",
    "test users in test: 1478\n",
    "test items in test: 3907\n",
    "train users in train: 666\n",
    "train items in train: 571\n",
    "-------------------\n",
    "Shape of test_cold: (1531, 5011)\n",
    "Shape of train_cold: (1531, 5011)\n",
    "Shape of X_cold: (1531, 5011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befcfc68-497f-414f-9e51-8b845cf78184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elicitation_by_tree_path_fixed_warm(\n",
    "    tree_model_class,\n",
    "    train, test, X,\n",
    "    matrix_warm,\n",
    "    idx_to_rid_cold, idx_to_rid_warm, idx_to_cid,\n",
    "    iteration=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Method 1 (revised): Fixed tree + fixed path elicitation, but tree is trained with warm + cold data.\n",
    "    User paths are still based only on initial cold ratings.\n",
    "\n",
    "    Parameters:\n",
    "    - tree_model_class: tree class, not instance\n",
    "    - train: cold users' initial ratings (lil_matrix)\n",
    "    - test: cold users' test set (lil_matrix)\n",
    "    - X: cold users' to-be-elicited ratings (lil_matrix)\n",
    "    - matrix_warm: warm users' full ratings (lil_matrix)\n",
    "    - idx_to_rid_cold, idx_to_rid_warm, idx_to_cid: index mappings\n",
    "    - iteration: number of elicitation rounds\n",
    "\n",
    "    Returns:\n",
    "    - rmse_list, mae_list\n",
    "    \"\"\"\n",
    "    from surprise import Dataset, Reader, SVD, accuracy\n",
    "\n",
    "    num_users, num_items = train.shape\n",
    "    train_static = train.tolil().copy()\n",
    "    train_copy = train.tolil().copy()\n",
    "    X_copy = X.tolil().copy()\n",
    "    user_paths = {u: None for u in range(num_users)}\n",
    "    rmse_list, mae_list = [], []\n",
    "\n",
    "    cid_to_idx = {v: k for k, v in idx_to_cid.items()}\n",
    "\n",
    "    # Step 1: Baseline evaluation with cold users\n",
    "    train_df = matrix_to_df_2(train_copy, idx_to_rid_cold, idx_to_cid)\n",
    "    reader = Reader(rating_scale=(1, 100))\n",
    "    algo = SVD()\n",
    "    trainset = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader).build_full_trainset()\n",
    "    algo.fit(trainset)\n",
    "\n",
    "    test_df = matrix_to_df_2(test, idx_to_rid_cold, idx_to_cid)\n",
    "    testset = Dataset.load_from_df(test_df[['user_id', 'item_id', 'rating']], reader).build_full_trainset().build_testset()\n",
    "    predictions = algo.test(testset)\n",
    "    rmse_list.append(accuracy.rmse(predictions, verbose=True))\n",
    "    mae_list.append(accuracy.mae(predictions, verbose=True))\n",
    "    print(\"✅ Baseline evaluation complete.\")\n",
    "\n",
    "    # Step 2: Train fixed tree with warm + cold\n",
    "    warm_df = matrix_to_full_df(matrix_warm, idx_to_rid_warm, idx_to_cid)\n",
    "    cold_df = matrix_to_full_df(train_static, idx_to_rid_cold, idx_to_cid)\n",
    "    x_df = pd.concat([warm_df, cold_df], ignore_index=False)\n",
    "\n",
    "    tree_model = tree_model_class(max_depth=iteration, min_instances=5)\n",
    "    tree_model.fit(x_df, x_df)\n",
    "    print(\"🌳 Tree training complete.\")\n",
    "\n",
    "    # Step 3: Initialize user paths\n",
    "    for u in range(num_users):\n",
    "        user_paths[u] = tree_model.root\n",
    "\n",
    "    # Step 4: Iterative elicitation\n",
    "    for i in range(iteration):\n",
    "        for u in range(num_users):\n",
    "            node = user_paths[u]\n",
    "            if node is None or node.is_leaf or node.attribute_name is None:\n",
    "                continue\n",
    "\n",
    "            item = node.attribute_name\n",
    "            if item not in cid_to_idx:\n",
    "                continue\n",
    "            item_idx = cid_to_idx[item]\n",
    "\n",
    "            rating_path = train_static[u, item_idx]  # from initial cold\n",
    "            rating_real = X_copy[u, item_idx]        # from ground truth\n",
    "\n",
    "            if rating_real > 0:\n",
    "                train_copy[u, item_idx] = rating_real\n",
    "                X_copy[u, item_idx] = 0\n",
    "\n",
    "            # Update path\n",
    "            if rating_path == 0:\n",
    "                user_paths[u] = node.children[2]\n",
    "            elif rating_path >= 50:\n",
    "                user_paths[u] = node.children[0]\n",
    "            else:\n",
    "                user_paths[u] = node.children[1]\n",
    "\n",
    "        # Step 5: Evaluate\n",
    "        train_df = matrix_to_df_2(train_copy, idx_to_rid_cold, idx_to_cid)\n",
    "        algo = SVD()\n",
    "        trainset = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader).build_full_trainset()\n",
    "        algo.fit(trainset)\n",
    "\n",
    "        test_df = matrix_to_df_2(test, idx_to_rid_cold, idx_to_cid)\n",
    "        testset = Dataset.load_from_df(test_df[['user_id', 'item_id', 'rating']], reader).build_full_trainset().build_testset()\n",
    "        predictions = algo.test(testset)\n",
    "        rmse_list.append(accuracy.rmse(predictions, verbose=True))\n",
    "        mae_list.append(accuracy.mae(predictions, verbose=True))\n",
    "        print(f\"✅ Iteration {i+1} complete.\")\n",
    "\n",
    "    return rmse_list, mae_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a2435-552a-49dd-9e97-22d0a9cf6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elicitation_by_tree_path_retrain_depth_warm(\n",
    "    tree_model_class,\n",
    "    train,              # lil_matrix: cold users' known ratings (1 item/user, updated over rounds)\n",
    "    test,               # lil_matrix: cold users' test ratings (30 ratings/user), fixed\n",
    "    X,                  # lil_matrix: cold users' \"hidden\" ratings, revealed 1 per round\n",
    "    matrix_warm,        # lil_matrix: warm users' full ratings\n",
    "    idx_to_rid_cold,    # cold user index → user_id\n",
    "    idx_to_rid_warm,    # warm user index → user_id\n",
    "    idx_to_cid,         # item index → item_id\n",
    "    iteration=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Method 2: Retrain tree each round using warm + cold user data.\n",
    "    Walk i steps for each cold user to reach a node and elicit a new rating.\n",
    "\n",
    "    Returns:\n",
    "    - rmse_list: RMSE after each iteration\n",
    "    - mae_list: MAE after each iteration\n",
    "    \"\"\"\n",
    "\n",
    "    num_users, num_items = train.shape\n",
    "    train_copy = train.tolil().copy()\n",
    "    X_copy = X.tolil().copy()\n",
    "    rmse_list, mae_list = [], []\n",
    "    cid_to_idx = {v: k for k, v in idx_to_cid.items()}\n",
    "\n",
    "    # Step 0: Baseline SVD using only cold-start (1 rating per user)\n",
    "    train_df = matrix_to_df_2(train_copy, idx_to_rid_cold, idx_to_cid)\n",
    "    reader = Reader(rating_scale=(1, 100))\n",
    "    algo = SVD()\n",
    "    trainset = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader).build_full_trainset()\n",
    "    algo.fit(trainset)\n",
    "\n",
    "    test_df = matrix_to_df_2(test, idx_to_rid_cold, idx_to_cid)\n",
    "    testset = Dataset.load_from_df(test_df[['user_id', 'item_id', 'rating']], reader).build_full_trainset().build_testset()\n",
    "    predictions = algo.test(testset)\n",
    "    rmse_list.append(accuracy.rmse(predictions, verbose=True))\n",
    "    mae_list.append(accuracy.mae(predictions, verbose=True))\n",
    "    print(\"✅ Baseline evaluation complete.\")\n",
    "\n",
    "    for i in range(iteration):\n",
    "        print(f\"\\n🔁 Iteration {i+1}/{iteration} (Tree depth = {i+1})\")\n",
    "\n",
    "        # Step 1: Prepare tree training data (warm + updated cold)\n",
    "        warm_df = matrix_to_full_df(matrix_warm, idx_to_rid_warm, idx_to_cid)\n",
    "        cold_df = matrix_to_full_df(train_copy, idx_to_rid_cold, idx_to_cid)\n",
    "        x_df = pd.concat([warm_df, cold_df], ignore_index=False)\n",
    "\n",
    "        tree_model = tree_model_class(max_depth=i+1, min_instances=5)\n",
    "        tree_model.fit(x_df, x_df)\n",
    "        print(\"🌳 Tree re-trained.\")\n",
    "\n",
    "        # Step 2: Walk tree and elicit a new rating for each cold user\n",
    "        for u in range(num_users):\n",
    "            node = tree_model.root\n",
    "            depth = 0\n",
    "\n",
    "            while node and not node.is_leaf and node.attribute_name and depth < i:\n",
    "                item = node.attribute_name\n",
    "                if item not in cid_to_idx:\n",
    "                    node = None\n",
    "                    break\n",
    "                item_idx = cid_to_idx[item]\n",
    "                rating = train_copy[u, item_idx]\n",
    "\n",
    "                if rating >= 50:\n",
    "                    node = node.children[0]  # Lovers\n",
    "                elif rating > 0:\n",
    "                    node = node.children[1]  # Haters\n",
    "                else:\n",
    "                    node = node.children[2]  # Unknowns\n",
    "\n",
    "                depth += 1\n",
    "\n",
    "            if node is None or node.attribute_name is None:\n",
    "                continue\n",
    "\n",
    "            item = node.attribute_name\n",
    "            if item not in cid_to_idx:\n",
    "                continue\n",
    "            item_idx = cid_to_idx[item]\n",
    "            rating = X_copy[u, item_idx]\n",
    "\n",
    "            if rating > 0:\n",
    "                train_copy[u, item_idx] = rating\n",
    "                X_copy[u, item_idx] = 0\n",
    "\n",
    "        # Step 3: Evaluate updated cold user SVD model\n",
    "        train_df = matrix_to_df_2(train_copy, idx_to_rid_cold, idx_to_cid)\n",
    "        algo = SVD()\n",
    "        trainset = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader).build_full_trainset()\n",
    "        algo.fit(trainset)\n",
    "\n",
    "        test_df = matrix_to_df_2(test, idx_to_rid_cold, idx_to_cid)\n",
    "        testset = Dataset.load_from_df(test_df[['user_id', 'item_id', 'rating']], reader).build_full_trainset().build_testset()\n",
    "        predictions = algo.test(testset)\n",
    "        rmse_list.append(accuracy.rmse(predictions, verbose=True))\n",
    "        mae_list.append(accuracy.mae(predictions, verbose=True))\n",
    "        print(f\"✅ Iteration {i+1} complete.\")\n",
    "\n",
    "    return rmse_list, mae_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebc183fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def elicitation_by_tree_path_retrain_skiped_warm(Tree, train, test, X, matrix_warm, idx_to_rid_cold, idx_to_rid_warm, idx_to_cid, iteration=5):\n",
    "    \"\"\"\n",
    "    Method 3 (revised): Retrain tree at each round. For each user, traverse from root down the tree.\n",
    "    At each level, if the item was already asked (stored in asked_items), go deeper.\n",
    "    Ask the first item the user hasn't been asked before.\n",
    "\n",
    "    Parameters:\n",
    "    - tree_model_class: class of the decision tree model (not instance)\n",
    "    - train, test, X: lil_matrix (known ratings, test set, and full matrix)\n",
    "    - idx_to_rid, idx_to_cid: index-to-ID mapping\n",
    "    - iteration: number of elicitation rounds\n",
    "\n",
    "    Returns:\n",
    "    - rmse_list, mae_list\n",
    "    \"\"\"\n",
    "\n",
    "    num_users, num_items = train.shape\n",
    "    train_copy = train.tolil().copy()\n",
    "    X_copy = X.tolil().copy()\n",
    "    rmse_list, mae_list = [], []\n",
    "    cid_to_idx = {v: k for k, v in idx_to_cid.items()}\n",
    "    asked_items = {u: set() for u in range(num_users)}  \n",
    "\n",
    "    # Step 0: Baseline evaluation\n",
    "    print(\"🔍 Evaluating baseline RMSE/MAE...\")\n",
    "    train_df = matrix_to_df_2(train_copy, idx_to_rid_cold, idx_to_cid)\n",
    "    reader = Reader(rating_scale=(1, 100))\n",
    "    data_r = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)\n",
    "    trainset = data_r.build_full_trainset()\n",
    "    algo = SVD()\n",
    "    algo.fit(trainset)\n",
    "    test_df = matrix_to_df_2(test, idx_to_rid_cold, idx_to_cid)\n",
    "    test_data = Dataset.load_from_df(test_df[['user_id', 'item_id', 'rating']], reader)\n",
    "    testset = test_data.build_full_trainset().build_testset()\n",
    "    predictions = algo.test(testset)\n",
    "    rmse_list.append(accuracy.rmse(predictions, verbose=True))\n",
    "    mae_list.append(accuracy.mae(predictions, verbose=True))\n",
    "    print(\"✅ Baseline evaluation complete.\")\n",
    "\n",
    "    for i in range(iteration):\n",
    "        print(f\"\\n🔁 Iteration {i+1}/{iteration} (skip asked items, walk from root)\")\n",
    "        # Retrain tree using current train_copy\n",
    "        warm_df = matrix_to_full_df(matrix_warm, idx_to_rid_warm, idx_to_cid)\n",
    "        coldK_df = matrix_to_full_df(train_copy, idx_to_rid_cold, idx_to_cid)\n",
    "        x_df = pd.concat([warm_df, coldK_df], ignore_index=False)\n",
    "\n",
    "        pct = Tree(max_depth=i + 1, min_instances=5)\n",
    "        pct.fit(x_df, x_df)\n",
    "        print(\"🌳 Tree re-trained.\")\n",
    "\n",
    "        for u in range(num_users):\n",
    "            node = pct.root\n",
    "\n",
    "            # Traverse down the tree until we find an unasked item\n",
    "            while node and not node.is_leaf and node.attribute_name:\n",
    "                item = node.attribute_name\n",
    "                if item not in cid_to_idx:\n",
    "                    node = None\n",
    "                    break\n",
    "\n",
    "                item_idx = cid_to_idx[item]\n",
    "\n",
    "                if item_idx in asked_items[u]:\n",
    "                    # Already asked, go deeper based on user's rating\n",
    "                    rating = train_copy[u, item_idx]\n",
    "                    if rating >= 50:\n",
    "                        node = node.children[0]\n",
    "                    elif rating > 0:\n",
    "                        node = node.children[1]\n",
    "                    else:\n",
    "                        node = node.children[2]\n",
    "                    continue\n",
    "\n",
    "                # First unasked item: try to add it to training\n",
    "                asked_items[u].add(item_idx)  \n",
    "                rating = X_copy[u, item_idx]\n",
    "                if rating > 0:\n",
    "                    train_copy[u, item_idx] = rating\n",
    "                    X_copy[u, item_idx] = 0\n",
    "                break  # only ask one item per user per iteration\n",
    "\n",
    "        # Step 3: Evaluate with SVD\n",
    "        print(\"📊 Evaluating after this iteration...\")\n",
    "        train_df = matrix_to_df_2(train_copy, idx_to_rid_cold, idx_to_cid)\n",
    "        data_r = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)\n",
    "        trainset = data_r.build_full_trainset()\n",
    "        algo.fit(trainset)\n",
    "        test_df = matrix_to_df_2(test, idx_to_rid_cold, idx_to_cid)\n",
    "        test_data = Dataset.load_from_df(test_df[['user_id', 'item_id', 'rating']], reader)\n",
    "        testset = test_data.build_full_trainset().build_testset()\n",
    "        predictions = algo.test(testset)\n",
    "        rmse_list.append(accuracy.rmse(predictions, verbose=True))\n",
    "        mae_list.append(accuracy.mae(predictions, verbose=True))\n",
    "        print(f\"✅ Iteration {i+1} complete.\")\n",
    "\n",
    "    return rmse_list, mae_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f8eb8-34c7-49c0-9cef-f495cb415dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def elicitation_by_tree_path_retrain_depth_tree_only_warm(\n",
    "    tree_model_class,\n",
    "    train, test, X,\n",
    "    matrix_warm,\n",
    "    idx_to_rid_cold, idx_to_rid_warm, idx_to_cid,\n",
    "    iteration=5\n",
    "):\n",
    "    \"\"\"\n",
    "    PCT 4 (enhanced version): Retrain a decision tree each round.\n",
    "    The same tree is used for both elicitation and recommendation.\n",
    "    Tree training uses cold users' known ratings + warm users' full ratings.\n",
    "\n",
    "    Returns:\n",
    "    - rmse_list: RMSE after each iteration\n",
    "    - mae_list: MAE after each iteration\n",
    "    \"\"\"\n",
    "    num_users, num_items = train.shape\n",
    "    train_copy = train.tolil().copy()  # cold users' ratings, updated each iteration\n",
    "    X_copy = X.tolil().copy()          # cold users' hidden ratings\n",
    "    cid_to_idx = {v: k for k, v in idx_to_cid.items()}\n",
    "\n",
    "    rmse_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    test_df = matrix_to_df_2(test, idx_to_rid_cold, idx_to_cid)\n",
    "    test_matrix = matrix_to_full_df(test, idx_to_rid_cold, idx_to_cid)\n",
    "\n",
    "    for i in range(iteration):\n",
    "        print(f\"\\n🔁 Iteration {i+1}/{iteration}\")\n",
    "\n",
    "        # Step 1: Prepare tree training data (cold + warm users)\n",
    "        warm_df = matrix_to_full_df(matrix_warm, idx_to_rid_warm, idx_to_cid)\n",
    "        cold_df = matrix_to_full_df(train_copy, idx_to_rid_cold, idx_to_cid)\n",
    "        x_df = pd.concat([warm_df, cold_df], ignore_index=False)\n",
    "\n",
    "        tree_model = tree_model_class(max_depth=i+1, min_instances=5)\n",
    "        tree_model.fit(x_df, x_df)\n",
    "        print(\"🌳 Tree trained.\")\n",
    "\n",
    "        # Step 2: Elicitation - each cold user walks the tree to elicit one new rating\n",
    "        for u in range(num_users):\n",
    "            node = tree_model.root\n",
    "            depth = 0\n",
    "\n",
    "            while node and not node.is_leaf and node.attribute_name and depth < i:\n",
    "                item = node.attribute_name\n",
    "                if item not in cid_to_idx:\n",
    "                    node = None\n",
    "                    break\n",
    "                item_idx = cid_to_idx[item]\n",
    "                rating = train_copy[u, item_idx]\n",
    "\n",
    "                if rating >= 50:\n",
    "                    node = node.children[0]  # Lovers\n",
    "                elif rating > 0:\n",
    "                    node = node.children[1]  # Haters\n",
    "                else:\n",
    "                    node = node.children[2]  # Unknowns\n",
    "\n",
    "                depth += 1\n",
    "\n",
    "            if node is None or node.attribute_name is None:\n",
    "                continue\n",
    "\n",
    "            item = node.attribute_name\n",
    "            if item not in cid_to_idx:\n",
    "                continue\n",
    "            item_idx = cid_to_idx[item]\n",
    "            rating = X_copy[u, item_idx]\n",
    "\n",
    "            if rating > 0:\n",
    "                train_copy[u, item_idx] = rating\n",
    "                X_copy[u, item_idx] = 0\n",
    "\n",
    "        print(\"❓ Elicitation done. Now evaluating...\")\n",
    "\n",
    "        # Step 3: Recommendation - use the trained tree to predict cold users' test ratings\n",
    "        pred_matrix = tree_model.predict(test_matrix)\n",
    "\n",
    "        y_true, y_pred = [], []\n",
    "\n",
    "        for row in test_df.itertuples():\n",
    "            uid, iid = row.user_id, row.item_id\n",
    "            true_rating = row.rating\n",
    "            pred_rating = pred_matrix.loc[uid, iid]\n",
    "\n",
    "            if not pd.isna(pred_rating):\n",
    "                y_true.append(true_rating)\n",
    "                y_pred.append(pred_rating)\n",
    "\n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse_list.append(rmse)\n",
    "        mae_list.append(mae)\n",
    "\n",
    "        print(f\"✅ Iteration {i+1} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
    "\n",
    "    return rmse_list, mae_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb06476",
   "metadata": {},
   "source": [
    "# X with artist only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb59e6-e463-4b71-9778-4f9f063fa850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pct1_artist_10 = elicitation_by_tree_path_fixed_warm(\n",
    "    tree_model_class=Tree,\n",
    "    train=train_cold_K_artist,\n",
    "    test=test_cold_artist,\n",
    "    X=X_cold_artist,\n",
    "    matrix_warm=matrix_warm,\n",
    "    idx_to_rid_cold=idx_to_rid_cold,\n",
    "    idx_to_rid_warm=idx_to_rid_warm,\n",
    "    idx_to_cid=idx_to_cid,\n",
    "    iteration=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499bf67d-c586-4a81-938f-0c31875901c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pct2_artist_10 = elicitation_by_tree_path_retrain_depth_warm(\n",
    "    tree_model_class=Tree,\n",
    "    train=train_cold_K_artist,\n",
    "    test=test_cold_artist,\n",
    "    X=X_cold_artist,\n",
    "    matrix_warm=matrix_warm,\n",
    "    idx_to_rid_cold=idx_to_rid_cold,\n",
    "    idx_to_rid_warm=idx_to_rid_warm,\n",
    "    idx_to_cid=idx_to_cid,\n",
    "    iteration=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00087c1e-f301-4106-ba48-2c7c04ad6d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pct3_artist_10 = elicitation_by_tree_path_retrain_skiped_warm(\n",
    "    Tree=Tree,\n",
    "    train=train_cold_K_artist,\n",
    "    test=test_cold_artist,\n",
    "    X=X_cold_artist,\n",
    "    matrix_warm=matrix_warm,\n",
    "    idx_to_rid_cold=idx_to_rid_cold,\n",
    "    idx_to_rid_warm=idx_to_rid_warm,\n",
    "    idx_to_cid=idx_to_cid,\n",
    "    iteration=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b817f-8861-4ca3-91c5-5c4612c2c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pct4_artist_10 = elicitation_by_tree_path_retrain_depth_tree_only_warm(\n",
    "    tree_model_class=Tree,\n",
    "    train=train_cold_K_artist,\n",
    "    test=test_cold_artist,\n",
    "    X=X_cold_artist,\n",
    "    matrix_warm=matrix_warm,\n",
    "    idx_to_rid_cold=idx_to_rid_cold,\n",
    "    idx_to_rid_warm=idx_to_rid_warm,\n",
    "    idx_to_cid=idx_to_cid,\n",
    "    iteration=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a4459a",
   "metadata": {},
   "source": [
    "# Alternative Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def i_entropy(matirx):\n",
    "    entropy = []\n",
    "    for item in range(0, matirx.shape[1]):\n",
    "        ratings = matirx.getcol(item).data\n",
    "        count_r = Counter(ratings)\n",
    "        len_rating = len(ratings)\n",
    "        ent = 0\n",
    "        for c in count_r:\n",
    "            ent -= (count_r[c]/len_rating)*math.log(count_r[c]/len_rating)\n",
    "        entropy.append(ent)\n",
    "    return (np.asarray(entropy))\n",
    "\n",
    "def i_entropy_0(matirx):\n",
    "    entropy = []\n",
    "    c_all = matirx.shape[0]\n",
    "    for item in range(0, matirx.shape[1]):\n",
    "        ratings = matirx.getcol(item).data\n",
    "        count_r = Counter(ratings)\n",
    "        count_r[0] = c_all - matirx.getcol(item).count_nonzero()\n",
    "        ent = 0\n",
    "        for c in count_r:\n",
    "            ent -= (count_r[c]/c_all)*math.log(count_r[c]/c_all)\n",
    "        entropy.append(ent)\n",
    "    return (np.asarray(entropy))\n",
    "\n",
    "def i_pop(matrix):\n",
    "    popularity = []\n",
    "    for item in range(0, matrix.shape[1]):\n",
    "        popularity.append(matrix.getcol(item).count_nonzero())\n",
    "        \n",
    "    return (np.asarray(popularity))\n",
    "\n",
    "\n",
    "\n",
    "def helf0(matrix):\n",
    "    log_U = np.log(matrix.shape[0])\n",
    "    lf = np.log(i_pop(matrix))/log_U\n",
    "    h = i_entropy_0(matrix)/np.log(5)\n",
    "    helf = (2* lf*h)/(lf+h)\n",
    "    return helf\n",
    "\n",
    "def i_variance(matirx):\n",
    "    c = matirx.copy()\n",
    "    c_2 = c.power(2)\n",
    "    E_2 = c_2.mean(0)\n",
    "    E = c.mean(0)\n",
    "    v = E_2 - np.sqrt(E)\n",
    "    return (np.asarray(v)).flatten()\n",
    "\n",
    "def i_random(matirx):\n",
    "    c = matirx.copy()\n",
    "    c_2 = c.power(2)\n",
    "    E_2 = c_2.mean(0)\n",
    "    E = c.mean(0)\n",
    "    v = E_2 - np.sqrt(E)\n",
    "    return (np.asarray(v)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae53937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "def elicitation_np_warm(matrix_warm,train,test,X,strategy,iteration,k,positive=False):\n",
    "    rmse,mae = [],[]\n",
    "    \n",
    "    # Performance before elicitation\n",
    "    train_df = matrix_to_df_2(train,idx_to_rid_cold,idx_to_cid)\n",
    "    reader = Reader(rating_scale=(1, 100))\n",
    "    data_r = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)\n",
    "    data_rr = data_r.build_full_trainset()\n",
    "    algo = SVD()\n",
    "    algo.fit(data_rr)\n",
    "    test_df = matrix_to_df_2(test,idx_to_rid_cold,idx_to_cid)\n",
    "    test_r = Dataset.load_from_df(test_df[['user_id', 'item_id', 'rating']], reader)\n",
    "    test_rr = test_r.build_full_trainset()\n",
    "    predictions= algo.test(test_rr.build_testset())\n",
    "    rmse.append(accuracy.rmse(predictions))\n",
    "    mae.append(accuracy.mae(predictions))\n",
    "    \n",
    "    c_u_dict = {}\n",
    "    all_items = list(range(0,train.shape[1]))\n",
    "    X_copy = (X.tolil()).copy()\n",
    "    warm_copy = matrix_warm.tolil().copy()\n",
    "    train_copy = (train.tolil()).copy()\n",
    "    \n",
    "\n",
    "    #scores = strategy(train_copy)\n",
    "    \n",
    "    for count in range(0,iteration-1):\n",
    "        x_matrix = sp.vstack([warm_copy, train_copy])\n",
    "        scores = strategy(x_matrix)\n",
    "        if positive:\n",
    "            ranking = np.argsort(-scores)\n",
    "        else:\n",
    "            ranking = np.argsort(scores)\n",
    "        for u in range(0, train.shape[0]):\n",
    "            if count ==0:\n",
    "                p_u = train_copy.getrow(u).nonzero()[1]\n",
    "                c_u = set_diff(all_items,p_u)\n",
    "                c_u_dict[u] = c_u.copy()\n",
    "            else:\n",
    "                c_u = c_u_dict[u].copy()                \n",
    "          \n",
    "            ranking_u = ranking[np.in1d(ranking,c_u)]\n",
    "            topk_u = ranking_u[:k]\n",
    "            c_u_dict[u] = set_diff(c_u,topk_u)\n",
    "            px_u = X_copy.getrow(u).nonzero()[1]\n",
    "            recom = set_intersection(topk_u,px_u)\n",
    "            for item in recom:                  \n",
    "                train_copy[u,item] = X[u,item]\n",
    "                X_copy[u,item] = 0\n",
    "        train_df = matrix_to_df_2(train_copy,idx_to_rid_cold,idx_to_cid)\n",
    "\n",
    "        reader = Reader(rating_scale=(1, 100))\n",
    "        data_r = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)\n",
    "        data_rr = data_r.build_full_trainset()\n",
    "        algo.fit(data_rr)\n",
    "        test_df = matrix_to_df(test,idx_to_rid_cold,idx_to_cid)\n",
    "        test_r = Dataset.load_from_df(test_df[['user_id', 'item_id', 'rating']], reader)\n",
    "        test_rr = test_r.build_full_trainset()\n",
    "        predictions= algo.test(test_rr.build_testset())\n",
    "        rmse.append(accuracy.rmse(predictions))\n",
    "        mae.append(accuracy.mae(predictions))\n",
    "        \n",
    "        print(count+1)\n",
    "    return rmse,mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef800636",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy0_artist_10 = elicitation_np_warm(\n",
    "    matrix_warm=matrix_warm,\n",
    "    train=train_cold_K_artist,\n",
    "    test=test_cold_artist,\n",
    "    X=X_cold_artist,\n",
    "    strategy=i_entropy_0,\n",
    "    iteration=20,\n",
    "    k=1,\n",
    "    positive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee4bd8-a0fc-409d-abe1-4faf1dcd22b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "helf0_artist_10 = elicitation_np_warm(\n",
    "    matrix_warm=matrix_warm,\n",
    "    train=train_cold_K_artist,\n",
    "    test=test_cold_artist,\n",
    "    X=X_cold_artist,\n",
    "    strategy=helf0,\n",
    "    iteration=20,\n",
    "    k=1,\n",
    "    positive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55b50f-abf6-4528-8ace-942daf0c7f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_artist_10 = elicitation_np_warm(\n",
    "    matrix_warm=matrix_warm,\n",
    "    train=train_cold_K_artist,\n",
    "    test=test_cold_artist,\n",
    "    X=X_cold_artist,\n",
    "    strategy=i_variance,\n",
    "    iteration=20,\n",
    "    k=1,\n",
    "    positive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07baf4bc-fb93-4d76-bf0f-a5ba4eb12999",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_artist_10 = elicitation_np_warm(\n",
    "    matrix_warm=matrix_warm,\n",
    "    train=train_cold_K_artist,\n",
    "    test=test_cold_artist,\n",
    "    X=X_cold_artist,\n",
    "    strategy=i_pop,\n",
    "    iteration=20,\n",
    "    k=1,\n",
    "    positive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0eeb5b",
   "metadata": {},
   "source": [
    "# Max performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a76fd-3aac-4664-ab1d-e33db447045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.arange(0,16) \n",
    "fig, (ax1) = plt.subplots(1)\n",
    "baseline_artist = 25.6939\n",
    "\n",
    "# fig.suptitle('Horizontally stacked subplots')\n",
    "ax1.plot(np.arange(len(pct1_artist_10[0])), pct1_artist_10[0], label='pct1_artist_10')\n",
    "ax1.plot(np.arange(len(pct2_artist_10[0])), pct2_artist_10[0], label='pct2_artist_10')\n",
    "ax1.plot(np.arange(len(pct3_artist_10[0])), pct3_artist_10[0], label='pct3_artist_10')\n",
    "ax1.plot(np.arange(len(pct4_artist_10[0])), pct4_artist_10[0], label='pct4_artist_10')\n",
    "ax1.axhline(y=baseline_artist, color='gray', linestyle='--', linewidth=2, label='Max performance artist')\n",
    "# ax1.plot(x, pct_2[0], label='pct_2'\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xticks(range(1, 21)) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a69d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.arange(0,16) \n",
    "fig, (ax1) = plt.subplots(1)\n",
    "baseline_artist = 25.6939\n",
    "\n",
    "# fig.suptitle('Horizontally stacked subplots')\n",
    "ax1.plot(np.arange(len(pct3_artist_10[0])), pct3_artist_10[0], label='pct3_artist_10')\n",
    "ax1.plot(np.arange(len(entropy0_artist_10[0])), entropy0_artist_10[0], label='entropy0_artist_10')\n",
    "ax1.plot(np.arange(len(helf0_artist_10[0])), helf0_artist_10[0], label='helf0_artist_10')\n",
    "ax1.plot(np.arange(len(variance_artist_10[0])), variance_artist_10[0], label='variance_artist_10')\n",
    "ax1.plot(np.arange(len(pop_artist_10[0])), pop_artist_10[0], label='pop_artist_10')\n",
    "ax1.axhline(y=baseline_artist, color='gray', linestyle='--', linewidth=2, label='Max performance artist')\n",
    "# ax1.plot(x, pct_2[0], label='pct_2'\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xticks(range(1, 21)) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aab10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.arange(0,16) \n",
    "fig, (ax1) = plt.subplots(1)\n",
    "baseline_artist = 25.6939\n",
    "\n",
    "# fig.suptitle('Horizontally stacked subplots')\n",
    "ax1.plot(np.arange(len(pct1_artist_10[0])), pct1_artist_10[0], label='pct1_artist_10')\n",
    "ax1.plot(np.arange(len(pct2_artist_10[0])), pct2_artist_10[0], label='pct2_artist_10')\n",
    "ax1.plot(np.arange(len(pct3_artist_10[0])), pct3_artist_10[0], label='pct3_artist_10')\n",
    "ax1.plot(np.arange(len(pct4_artist_10[0])), pct4_artist_10[0], label='pct4_artist_10')\n",
    "ax1.plot(np.arange(len(entropy0_artist_10[0])), entropy0_artist_10[0], label='entropy0_artist_10')\n",
    "ax1.plot(np.arange(len(helf0_artist_10[0])), helf0_artist_10[0], label='helf0_artist_10')\n",
    "ax1.plot(np.arange(len(variance_artist_10[0])), variance_artist_10[0], label='variance_artist_10')\n",
    "ax1.plot(np.arange(len(pop_artist_10[0])), pop_artist_10[0], label='pop_artist_10')\n",
    "ax1.axhline(y=baseline_artist, color='gray', linestyle='--', linewidth=2, label='Max performance artist')\n",
    "# ax1.plot(x, pct_2[0], label='pct_2'\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xticks(range(1, 21)) \n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
